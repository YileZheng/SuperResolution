{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd\n",
    "import os\n",
    "import math\n",
    "import random\n",
    "import cv2\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import quandl\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torch.utils.data as Data\n",
    "\n",
    "import ColorLog as debug\n",
    "\n",
    "from load_data import labelFpsDataLoader, labelTestDataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "DEVICE = \"cuda\"\n",
    "provinces = [\"皖\", \"沪\", \"津\", \"渝\", \"冀\", \"晋\", \"蒙\", \"辽\", \"吉\", \"黑\", \"苏\", \"浙\", \"京\", \"闽\", \"赣\", \"鲁\", \"豫\", \"鄂\", \"湘\", \"粤\", \"桂\", \"琼\", \"川\", \"贵\", \"云\", \"藏\", \"陕\", \"甘\", \"青\", \"宁\", \"新\", \"警\", \"学\", \"O\"]\n",
    "alphabets = ['A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'J', 'K', 'L', 'M', 'N', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W',\n",
    "             'X', 'Y', 'Z', 'O']\n",
    "ads = ['A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'J', 'K', 'L', 'M', 'N', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X',\n",
    "       'Y', 'Z', '0', '1', '2', '3', '4', '5', '6', '7', '8', '9', 'O']\n",
    "\n",
    "NUM_PROV = len(provinces)\n",
    "NUM_ALPB = len(alphabets)\n",
    "NUM_ADS = len(ads)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data loading \n",
    "# image size 720x1160x3 \n",
    "\n",
    "\n",
    "image_types = (\".jpg\", \".jpeg\", \".png\", \".bmp\", \".tif\", \".tiff\")\n",
    "\n",
    "\n",
    "def list_images(basePath, contains=None):\n",
    "    # return the set of files that are valid\n",
    "    return list_files(basePath, validExts=image_types, contains=contains)\n",
    "\n",
    "\n",
    "def list_files(basePath, validExts=None, contains=None):\n",
    "    # loop over the directory structure\n",
    "    for (rootDir, dirNames, filenames) in os.walk(basePath):\n",
    "        # loop over the filenames in the current directory\n",
    "        for filename in filenames:\n",
    "            # if the contains string is not none and the filename does not contain\n",
    "            # the supplied string, then ignore the file\n",
    "            if contains is not None and filename.find(contains) == -1:\n",
    "                continue\n",
    "\n",
    "            # determine the file extension of the current file\n",
    "            ext = filename[filename.rfind(\".\"):].lower()\n",
    "\n",
    "            # check to see if the file is an image and should be processed\n",
    "            if validExts is None or ext.endswith(validExts):\n",
    "                # construct the path to the image and yield it\n",
    "                imagePath = os.path.join(rootDir, filename)\n",
    "                yield imagePath\n",
    "                \n",
    "class labelFpsDataLoader(Data.Dataset):\n",
    "    def __init__(self, img_dir, imgSize, is_transform=None):\n",
    "        self.img_dir = img_dir\n",
    "        self.img_paths = []\n",
    "        for i in range(len(img_dir)):\n",
    "            self.img_paths += [el for el in list_images(img_dir[i])]\n",
    "        # self.img_paths = os.listdir(img_dir)\n",
    "        # print self.img_paths\n",
    "        self.img_size = imgSize\n",
    "        self.is_transform = is_transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.img_paths)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        img_name = self.img_paths[index]\n",
    "        img = cv2.imread(img_name)\n",
    "        # img = img.astype('float32')\n",
    "        lbl = img_name.split('/')[-1].rsplit('.', 1)[0].split('-')[-3]\n",
    "\n",
    "        iname = img_name.rsplit('/', 1)[-1].rsplit('.', 1)[0].split('-')\n",
    "        # fps = [[int(eel) for eel in el.split('&')] for el in iname[3].split('_')]\n",
    "        # leftUp, rightDown = [min([fps[el][0] for el in range(4)]), min([fps[el][1] for el in range(4)])], [\n",
    "        #     max([fps[el][0] for el in range(4)]), max([fps[el][1] for el in range(4)])]\n",
    "        \n",
    "#         print(debug.DEBUG,iname)\n",
    "        \n",
    "        [leftUp, rightDown] = [[int(eel) for eel in el.split('&')] for el in iname[2].split('_')]\n",
    "        ori_w, ori_h = [float(int(el)) for el in [img.shape[1], img.shape[0]]]\n",
    "        new_labels = [(leftUp[0] + rightDown[0]) / (2 * ori_w), (leftUp[1] + rightDown[1]) / (2 * ori_h),\n",
    "                      (rightDown[0] - leftUp[0]) / ori_w, (rightDown[1] - leftUp[1]) / ori_h]\n",
    "        croppedImage = img[leftUp[1]:rightDown[1],leftUp[0]:rightDown[0]]\n",
    "        resizedImage = cv2.resize(croppedImage, self.img_size)\n",
    "#         cv2.imshow('plate',resizedImage)\n",
    "#         cv2.waitKey(0)\n",
    "        print(resizedImage.shape)\n",
    "        resizedImage = np.transpose(resizedImage, (2,0,1))\n",
    "        resizedImage = resizedImage.astype('float32')\n",
    "        resizedImage /= 255.0\n",
    "        \n",
    "#         cv2.imshow('plate',np.transpose(resizedImage, (1,2,0)))\n",
    "#         cv2.waitKey(0)\n",
    "        \n",
    "        return resizedImage, new_labels, lbl, img_name, iname\n",
    "    \n",
    "class labelLoader(Data.Dataset):\n",
    "    def __init__(self, img_dir, imgSize, is_transform=None):\n",
    "        self.img_dir = img_dir\n",
    "        self.img_paths = []\n",
    "        for i in range(len(img_dir)):\n",
    "            self.img_paths += [el for el in list_images(img_dir[i])]\n",
    "        # self.img_paths = os.listdir(img_dir)\n",
    "        # print self.img_paths\n",
    "        self.img_size = imgSize\n",
    "        self.is_transform = is_transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.img_paths)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        img_name = self.img_paths[index]\n",
    "#         img = cv2.imread(img_name)\n",
    "#         # img = img.astype('float32')\n",
    "#         resizedImage = cv2.resize(img, self.img_size)\n",
    "#         resizedImage = np.transpose(resizedImage, (2,0,1))\n",
    "#         resizedImage = resizedImage.astype('float32')\n",
    "#         resizedImage /= 255.0\n",
    "        lbl = img_name.split('/')[-1].rsplit('.', 1)[0].split('-')[-3]\n",
    "\n",
    "        iname = img_name.rsplit('/', 1)[-1].rsplit('.', 1)[0].split('-')\n",
    "        # fps = [[int(eel) for eel in el.split('&')] for el in iname[3].split('_')]\n",
    "        # leftUp, rightDown = [min([fps[el][0] for el in range(4)]), min([fps[el][1] for el in range(4)])], [\n",
    "        #     max([fps[el][0] for el in range(4)]), max([fps[el][1] for el in range(4)])]\n",
    "        \n",
    "#         print(debug.DEBUG,iname)\n",
    "        \n",
    "        [leftUp, rightDown] = [[int(eel) for eel in el.split('&')] for el in iname[2].split('_')]\n",
    "#         ori_w, ori_h = [float(int(el)) for el in [img.shape[1], img.shape[0]]]\n",
    "#         new_labels = [(leftUp[0] + rightDown[0]) / (2 * ori_w), (leftUp[1] + rightDown[1]) / (2 * ori_h),\n",
    "#                       (rightDown[0] - leftUp[0]) / ori_w, (rightDown[1] - leftUp[1]) / ori_h]\n",
    "\n",
    "        return lbl, img_name, iname"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\u001b[37;1mINFO\u001b[0m] -- start loading data...\n",
      "[\u001b[37;1mINFO\u001b[0m] -- finish loading\n",
      "(96, 252, 3)\n",
      "(3, 96, 252)\n",
      "(96, 252, 3)\n"
     ]
    }
   ],
   "source": [
    "path = \"CCPD2019/ccpd_base\"\n",
    "imgsize = (480,480)\n",
    "platesize = (252,96)\n",
    "print(debug.INFO+\"start loading data...\")\n",
    "dst = labelFpsDataLoader([path],platesize)\n",
    "# dst = labelLoader([path],imgsize)\n",
    "print(debug.INFO+\"finish loading\")\n",
    "# sizes = []\n",
    "# for _,_,iname in dst:\n",
    "#     [leftUp, rightDown] = [[int(eel) for eel in el.split('&')] \n",
    "#                        for el in iname[2].split('_')]\n",
    "#     sizes.append([leftUp[0]-rightDown[0],leftUp[1]-rightDown[1]])\n",
    "\n",
    "# mean, std = np.mean(sizes, axis=0),np.std(sizes, axis=0)\n",
    "# print(mean,std)\n",
    "#     [-252.62252245  -94.97832957] [61.46956803 27.42365916]\n",
    "for img,_,_,_,iname in dst:\n",
    "    print(img.shape)\n",
    "    img =np.transpose(img, (1,2,0))\n",
    "    \n",
    "    print(img.shape)\n",
    "    cv2.imshow('plate',img)\n",
    "    cv2.waitKey(0)\n",
    "    break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DigitRecog(nn.Module): \n",
    "    def __init__(self, ):\n",
    "\n",
    "        super(DigitRecog, self).__init__()\n",
    "        self.name = \"DigitRecog\"\n",
    "        \n",
    "        self.stack1 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=3, out_channels=64, kernel_size=3, padding=2),\n",
    "            nn.BatchNorm2d(num_features=192),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(in_channels=64, out_channels=64, kernel_size=3, padding=2),\n",
    "            nn.BatchNorm2d(num_features=192),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=1, padding=1),\n",
    "            nn.Dropout(0.2)\n",
    "        )\n",
    "        self.stack2 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=64, out_channels=128, kernel_size=3, padding=2),\n",
    "            nn.BatchNorm2d(num_features=192),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(in_channels=128, out_channels=128, kernel_size=3, padding=2),\n",
    "            nn.BatchNorm2d(num_features=192),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=1, padding=1),\n",
    "            nn.Dropout(0.2)\n",
    "        )\n",
    "        self.stack3 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=128, out_channels=256, kernel_size=3, padding=2),\n",
    "            nn.BatchNorm2d(num_features=192),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(in_channels=256, out_channels=256, kernel_size=3, padding=2),\n",
    "            nn.BatchNorm2d(num_features=192),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=1, padding=1),\n",
    "            nn.Dropout(0.2)\n",
    "        )\n",
    "        self.stack4 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=256, out_channels=512, kernel_size=3, padding=2),\n",
    "            nn.BatchNorm2d(num_features=192),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(in_channels=512, out_channels=512, kernel_size=3, padding=2),\n",
    "            nn.BatchNorm2d(num_features=192),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=1, padding=1),\n",
    "            nn.Dropout(0.2)\n",
    "        )\n",
    "        self.stack5 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=512, out_channels=512, kernel_size=3, padding=2),\n",
    "            nn.BatchNorm2d(num_features=192),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(in_channels=512, out_channels=512, kernel_size=3, padding=2),\n",
    "            nn.BatchNorm2d(num_features=192),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=1, padding=1),\n",
    "            nn.Dropout(0.2)\n",
    "        )\n",
    "        self.activations = nn.Sequential(\n",
    "            self.stack1,\n",
    "            self.stack2,\n",
    "            self.stack3,\n",
    "            self.stack4,\n",
    "            self.stack5\n",
    "        )\n",
    "        \n",
    "        \n",
    "        self.classifier1 = nn.Sequential(\n",
    "            nn.Linear(23232, 4096),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(4096, 1000),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(1000, NUM_PROV)\n",
    "        )\n",
    "        self.classifier2 = nn.Sequential(\n",
    "            nn.Linear(23232, 4096),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(4096, 1000),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(1000, NUM_ALPB)\n",
    "        )\n",
    "        \n",
    "        self.classifier3 = nn.Sequential(\n",
    "            nn.Linear(23232, 4096),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(4096, 1000),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(1000, NUM_ADS)\n",
    "        )\n",
    "        self.classifier4 = nn.Sequential(\n",
    "            nn.Linear(23232, 4096),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(4096, 1000),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(1000, NUM_ADS)\n",
    "        )\n",
    "        self.classifier5 = nn.Sequential(\n",
    "            nn.Linear(23232, 4096),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(4096, 1000),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(1000, NUM_ADS)\n",
    "        )\n",
    "        self.classifier6 = nn.Sequential(\n",
    "            nn.Linear(23232, 4096),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(4096, 1000),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(1000, NUM_ADS)\n",
    "        )\n",
    "        self.classifier7 = nn.Sequential(\n",
    "            nn.Linear(23232, 4096),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(4096, 1000),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(1000, NUM_ADS)\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        x = self.activations(x)\n",
    "        x = x.view(x.size(0),-1)\n",
    "        prov = self.classifier1(x)\n",
    "        alpb = self.classifier2(x)\n",
    "        ads1 = self.classifier3(x)\n",
    "        ads2 = self.classifier4(x)\n",
    "        ads3 = self.classifier5(x)\n",
    "        ads4 = self.classifier6(x)\n",
    "        ads5 = self.classifier7(x)\n",
    "        return prov,alpb,ads1,ads2,ads3,ads4,ads5   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def train_model(model, trainloader, criterion, optimizer, num_epochs=25):\n",
    "    # since = time.time()\n",
    "    for epoch in range(epoch_start, num_epochs):\n",
    "        lossAver = []\n",
    "        model.train(True)\n",
    "        lrScheduler.step()\n",
    "        start = time()\n",
    "\n",
    "        for i, (XI, Y, labels, ims) in enumerate(trainloader):\n",
    "            if not len(XI) == batchSize:\n",
    "                continue\n",
    "\n",
    "            YI = [[int(ee) for ee in el.split('_')[:7]] for el in labels]\n",
    "            Y = np.array([el.numpy() for el in Y]).T\n",
    "            if use_gpu:\n",
    "                x = Variable(XI.cuda(0))\n",
    "                y = Variable(torch.FloatTensor(Y).cuda(0), requires_grad=False)\n",
    "            else:\n",
    "                x = Variable(XI)\n",
    "                y = Variable(torch.FloatTensor(Y), requires_grad=False)\n",
    "            # Forward pass: Compute predicted y by passing x to the model\n",
    "\n",
    "            try:\n",
    "                fps_pred, y_pred = model(x)\n",
    "            except:\n",
    "                continue\n",
    "                \n",
    "            # Compute and print loss\n",
    "            loss = 0.0\n",
    "            loss += 0.8 * nn.L1Loss().cuda()(fps_pred[:][:2], y[:][:2])\n",
    "            loss += 0.2 * nn.L1Loss().cuda()(fps_pred[:][2:], y[:][2:])\n",
    "            for j in range(7):\n",
    "                l = Variable(torch.LongTensor([el[j] for el in YI]).cuda(0))\n",
    "                loss += criterion(y_pred[j], l)\n",
    "\n",
    "            # Zero gradients, perform a backward pass, and update the weights.\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            try:\n",
    "                lossAver.append(loss.data[0])\n",
    "            except:\n",
    "                pass\n",
    "\n",
    "            if i % 50 == 1:\n",
    "                with open(args['writeFile'], 'a') as outF:\n",
    "                    outF.write('train %s images, use %s seconds, loss %s\\n' % (i*batchSize, time(    ) - start, sum(lossAver) / len(lossAver) if len(lossAver)>0 else 'NoLoss'))\n",
    "                torch.save(model.state_dict(), storeName)\n",
    "        print ('%s %s %s\\n' % (epoch, sum(lossAver) / len(lossAver), time()-start))\n",
    "        model.eval()\n",
    "        count, correct, error, precision, avgTime = eval(model, testDirs)\n",
    "        with open(args['writeFile'], 'a') as outF:\n",
    "            outF.write('%s %s %s\\n' % (epoch, sum(lossAver) / len(lossAver), time() - start))\n",
    "            outF.write('*** total %s error %s precision %s avgTime %s\\n' % (count, error, precisi    on, avgTime))\n",
    "        torch.save(model.state_dict(), storeName + str(epoch))\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "# optimizer_conv = optim.RMSprop(model_conv.parameters(), lr=0.01, momentum=0.9)\n",
    "optimizer_conv = optim.SGD(model_conv.parameters(), lr=0.001, momentum=0.9)\n",
    "dst = labelFpsDataLoader(trainDirs, imgSize)\n",
    "trainloader = Data.DataLoader(dst, batch_size=batchSize, shuffle=True, num_workers=8)\n",
    "lrScheduler = lr_scheduler.StepLR(optimizer_conv, step_size=5, gamma=0.1)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tradt",
   "language": "python",
   "name": "tradt"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
